# -*- coding: utf-8 -*-
"""FinalProjectMain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UKRGmp6DDjj3u_UiL7x_8-gsOly8xbqG

# CIS 520 Final Project

**DengAI**

**Pranav Panganamamula**

**Dinesh Jagai**

**Julian Schnitzler**

## Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib
import math
import matplotlib.pyplot as plt
import sklearn
import numpy as np
from collections import deque
import tensorflow as tf
from tensorflow import keras
from sklearn import linear_model
from sklearn import ensemble
from sklearn import kernel_ridge
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
import random
import pickle
# %matplotlib inline

from google.colab import drive #(NO NEED TO RUN THIS AGAIN)
drive.mount('/content/gdrive')

import warnings
warnings.filterwarnings('ignore')

np.set_printoptions(suppress=True, formatter={'float_kind':'{:f}'.format})

"""## Import Data"""

# Load in the data (N.b - the data can either be raw (with missing values), mean imputed, regression imputed)
# Code is equivalent to:
'''
X_train = pd.read_csv("dengue_features_train_mean_imputed.csv")
y_train = pd.read_csv("dengue_labels_train.csv")
X_test = pd.read_csv("dengue_features_test_mean_imputed.csv")

path_regression_train = "dengue_features_train_mean_imputed.csv" 
path_regression_test = "dengue_features_test_mean_imputed.csv"
X_train_regression_imputed = pd.read_csv(path_regression_train)
X_test_regression_imputed = pd.read_csv(path_regression_test)
'''

X_train = pd.read_csv("https://raw.githubusercontent.com/dineshjagai/CIS-520-Final-Project/master/data/dengue_features_train.csv")
X_test = pd.read_csv("https://raw.githubusercontent.com/dineshjagai/CIS-520-Final-Project/master/data/dengue_features_test.csv")

X_train_base_imputed = pd.read_csv("https://raw.githubusercontent.com/dineshjagai/CIS-520-Final-Project/master/data_mean_imputated/dengue_features_train_mean_imputed.csv?token=AHUIMVLMT4UVKYXBRKUQHA256Z2NG")
y_train = pd.read_csv("https://raw.githubusercontent.com/dineshjagai/CIS-520-Final-Project/master/data/dengue_labels_train.csv?token=AHUIMVJZ2V5HBUEJXHFV6HK56Z2ZI")

X_test_base_imputed = pd.read_csv("https://raw.githubusercontent.com/dineshjagai/CIS-520-Final-Project/master/data_mean_imputated/dengue_features_test_mean_imputed.csv?token=AHUIMVPYWGROYT5V3QFPBZ256Z256")

"""# Regression Imputation"""

data_train = pd.DataFrame.join(X_train_base_imputed, y_train, rsuffix = "_r")
data_train = data_train.drop(['city_r', 'year_r', 'weekofyear_r'], axis=1)

data_train_missing = pd.DataFrame.join(X_train, y_train, rsuffix = "_r")
data_train_regressed = data_train_missing.copy()
data_train_missing = data_train_missing.drop(['city_r', 'year_r', 'weekofyear_r'], axis=1)

# Extract city specific data and drop 'city' column from DataFames
data_test_sj = X_test_base_imputed[X_test_base_imputed['city'] == 'sj']
data_test_iq = X_test_base_imputed[X_test_base_imputed['city'] == 'iq']
data_test_sj = data_test_sj.drop(['city'], axis=1)
data_test_iq = data_test_iq.drop(['city'], axis=1)

data_train_sj = data_train[data_train['city'] == 'sj']
data_train_iq = data_train[data_train['city'] == 'iq']
data_train_sj = data_train_sj.drop(['city'], axis=1)
data_train_iq = data_train_iq.drop(['city'], axis=1)

data_train_missing_sj = data_train_missing[data_train_missing['city'] == 'sj']
data_train_missing_iq = data_train_missing[data_train_missing['city'] == 'iq']
data = data_train_missing_sj.copy()
data2 = data_train_missing_iq.copy()

data_train_missing_sj = data_train_missing_sj.drop(['city'], axis=1)
data_train_missing_iq = data_train_missing_iq.drop(['city'], axis=1)

data_test_missing_sj = X_test[X_test['city'] == 'sj']
data_test_missing_iq = X_test[X_test['city'] == 'iq']
data3 = data_test_missing_sj.copy()
data4 = data_test_missing_iq.copy()

data_test_missing_sj = data_test_missing_sj.drop(['city'], axis=1)
data_test_missing_iq = data_test_missing_iq.drop(['city'], axis=1)

X_test.head()
# process data

# Drop irrelevant features 
y_train_sj = data_train_sj['total_cases']
X_train_sj = data_train_sj.drop(['total_cases', 'week_start_date', 'year', 'weekofyear'], axis=1)

# Convert to numpy array 
X_train_sj_np = X_train_sj.to_numpy()

y_train_sj_np = y_train_sj.to_numpy()
y_train_sj_np = np.reshape(y_train_sj_np, (len(y_train_sj_np), 1))

y_train_iq = data_train_iq['total_cases']
X_train_iq = data_train_iq.drop(['total_cases', 'week_start_date', 'year', 'weekofyear'], axis=1)

X_train_iq_np = X_train_iq.to_numpy()
y_train_iq_np = y_train_iq.to_numpy()
y_train_iq_np = np.reshape(y_train_iq_np, (len(y_train_iq_np), 1))

X_test_sj = data_test_sj.drop(['week_start_date', 'year', 'weekofyear'], axis=1)
X_test_sj_np = X_test_sj.to_numpy()

X_test_iq = data_test_iq.drop(['week_start_date', 'year', 'weekofyear'], axis=1)
X_test_iq_np = X_test_iq.to_numpy()

########################################################################

X_train_missing_sj = data_train_missing_sj.drop(['total_cases', 'week_start_date', 'year', 'weekofyear'], axis=1)
X_train_missing_sj_np = X_train_missing_sj.to_numpy()

X_train_missing_iq = data_train_missing_iq.drop(['total_cases', 'week_start_date', 'year', 'weekofyear'], axis=1)
X_train_missing_iq_np = X_train_missing_iq.to_numpy()

X_test_missing_sj = data_test_missing_sj.drop(['week_start_date', 'year', 'weekofyear'], axis=1)
X_test_missing_sj_np = X_test_missing_sj.to_numpy()

X_test_missing_iq = data_test_missing_iq.drop(['week_start_date', 'year', 'weekofyear'], axis=1)
X_test_missing_iq_np = X_test_missing_iq.to_numpy()

#X_train_sj.head()

"""# Perform Regression Imputation"""

def regressedImpute(X_baseImputed, X_miss):
    '''
    Returns :
        X_imputed which has mean of the linearly regressed value instead of the missing values and same shape as X_miss.
    if computePerFeatureStatistics is True, also:
        list of Frobenius norms of difference between reconstructions and original data (without missing values) calculated after each imputing each column.
        list of accuracies on test set of Logistic Regression classifier trained on imputed data after each imputing each column.
    '''

    X_imputed = X_baseImputed.copy()
    (n, p) = X_imputed.shape

    # We do a linear regression based imputation here, for each column, train a classifier to predict its value based on values of other features and
    # replace the NaN with the predicted values. 
    
    for i in range(p):
        rows = np.isnan(X_miss[:, i]) 
        regress_rows = X_baseImputed[~rows]
        data = np.delete(regress_rows, i, 1)
        regress_y = X_baseImputed[~rows, i]
        missing = X_baseImputed[rows]
        miss = np.delete(missing, i, 1) 
        if (len(miss) == 0):
            continue
        clf = LinearRegression().fit(data, regress_y) 
        y_predict = clf.predict(miss)
        index = rows.nonzero()[0]
        np.put(X_imputed[:, i], index, y_predict) 

    return X_imputed

# Obtain baseline regression imputed features
X_regressed_sj = regressedImpute(X_train_sj_np, X_train_missing_sj_np)
X_regressed_iq = regressedImpute(X_train_iq_np, X_train_missing_iq_np)

X_test_regressed_sj = regressedImpute(X_test_sj_np, X_test_missing_sj_np)
X_test_regressed_iq = regressedImpute(X_test_iq_np, X_test_missing_iq_np)

epochs = 5

# Improve imputation by repeating process using the previous imputation as the baseline imputation for the next imputing
for i in range(epochs):
    X_copy_sj = X_regressed_sj.copy()
    X_copy_iq = X_regressed_iq.copy()
    X_test_copy_sj = X_test_regressed_sj.copy()
    X_test_copy_iq = X_test_regressed_iq.copy()
    
    X_regressed_sj = regressedImpute(X_copy_sj, X_train_missing_sj_np)
    X_regressed_iq = regressedImpute(X_copy_iq, X_train_missing_iq_np)
    
    X_test_regressed_sj = regressedImpute(X_test_copy_sj, X_test_missing_sj_np)
    X_test_regressed_iq = regressedImpute(X_test_copy_iq, X_test_missing_iq_np)

# Set up list of features
column_list = list(data_train_regressed.columns.values.tolist()) 
column_list.remove('city')
column_list.remove('year')
column_list.remove('weekofyear')
column_list.remove('week_start_date')
column_list.remove('total_cases')
column_list.remove('city_r')
column_list.remove('year_r')
column_list.remove('weekofyear_r')

# Create city DataFrames for the imputed values
sj_df = pd.DataFrame(X_regressed_sj, columns=column_list)
iq_df = pd.DataFrame(X_regressed_iq, columns=column_list)

sj_test_df = pd.DataFrame(X_test_regressed_sj, columns=column_list)
iq_test_df = pd.DataFrame(X_test_regressed_iq, columns=column_list)

# Update original DataFrames with new imputed values
data = data.reset_index(drop=True)
data2 = data2.reset_index(drop=True)
data.update(sj_df)
data2.update(iq_df)

data3 = data3.reset_index(drop=True)
data4 = data4.reset_index(drop=True)
data3.update(sj_test_df)
data4.update(iq_test_df)

# Concatenate city datasets
data_reg = pd.concat([data, data2])
data_reg = data_reg.reset_index(drop=True)

data_test_reg = pd.concat([data3, data4])
data_test_reg = pd.concat([data3, data4])

#data_test_reg.head() 
data_test_reg.head()

# Dataframe for regressed imputed data exported to csv file and uploaded to Github separately on local machine

X_train_regression_imputed = pd.read_csv("https://raw.githubusercontent.com/dineshjagai/CIS-520-Final-Project/master/data_regressed_imputed/dengue_features_train_regressed_imputed.csv?token=AK72LX6E665WSTCSLAPM74K57A4DA")
X_test_regression_imputed = pd.read_csv("https://raw.githubusercontent.com/dineshjagai/CIS-520-Final-Project/master/data_regressed_imputed/dengue_features_test_regressed_imputed.csv?token=AK72LX33SSHA7LSYAIVDTPC57A4HG")

"""# Seperate The Data iInt"""

# Seperate the data into their respective positions
X_data_train_sj = X_train_base_imputed[X_train_base_imputed['city'] == 'sj']
X_data_train_iq = X_train_base_imputed[X_train_base_imputed['city'] == 'iq']  
y_data_train_sj = y_train[y_train['city'] == 'sj']
y_data_train_iq = y_train[y_train['city'] == 'iq']
X_data_test_sj  = X_test[X_test['city'] == 'sj']
X_data_test_iq  = X_test[X_test['city'] == 'iq']

# Regressed data 
X_train_regression_imputed_sj = X_train_regression_imputed[X_train_regression_imputed['city'] == 'sj']
X_train_regression_imputed_iq = X_train_regression_imputed[X_train_regression_imputed['city'] == 'iq']
X_test_regression_imputed_sj = X_test_regression_imputed[X_test_regression_imputed['city'] == 'sj']
X_test_regression_imputed_iq = X_test_regression_imputed[X_test_regression_imputed['city'] == 'iq']

# drop the uncessary parts
y_data_train_sj = y_data_train_sj.drop(["city","year", "weekofyear"], axis=1)
y_data_train_iq = y_data_train_iq.drop(["city","year", "weekofyear"], axis=1)

# put features and label into one dataset (needed e.g. for correlation)
data_train_sj = pd.concat([X_data_train_sj, y_data_train_sj], axis=1)
data_train_iq = pd.concat([X_data_train_iq, y_data_train_iq], axis=1)

# Training (Mean Imputed Values)
X_data_train_sj = X_data_train_sj.drop(["city","year", "week_start_date", "weekofyear"], axis=1)
X_data_train_iq = X_data_train_iq.drop(["city","year", "week_start_date", "weekofyear"], axis=1)

# Testing (Mean Imputed Values)
X_data_test_sj = X_data_test_sj.drop(["city","year", "week_start_date", "weekofyear"], axis=1)
X_data_test_iq = X_data_test_iq.drop(["city","year", "week_start_date", "weekofyear"], axis=1)

#  Training Regression Imputed Values 
X_train_regression_imputed_sj = X_train_regression_imputed_sj.drop(["city","year", "week_start_date", "weekofyear", "total_cases"], axis=1)
X_train_regression_imputed_iq = X_train_regression_imputed_iq.drop(["city","year", "week_start_date", "weekofyear", "total_cases"], axis=1)

# Testing (Regression Imputed Values)
X_test_regression_imputed_sj = X_test_regression_imputed_sj.drop(["city","year", "week_start_date", "weekofyear"], axis=1)
X_test_regression_imputed_iq = X_test_regression_imputed_iq.drop(["city","year", "week_start_date", "weekofyear"], axis=1)

"""# Visualize The Data"""

#  view all the Data (As a style )
# X_data_train_sj.style
# X_data_train_iq .style
# y_data_train_iq.style
# y_data_train_sj.style
# X_train_regression_imputed_sj.style
# X_train_regression_imputed_iq.style

"""# Histogram Of Cases For IQ and SJ in training Set"""

y_data_train_iq.hist()

y_data_train_sj.hist()

# Table containing all of the data 
# print(X_train.as_matrix)



# X_data_train.style
# X_data_train.head()
# X_data_train.describe()
# X_data_train.info()
# X_data_train.shape
# pd.plotting.scatter_matrix(data_train)

"""# Plot Of the Correlation Matrix For IQ"""

# plot correlation matrix for Iq  (Mean Imputed Values)
data_train_iq['total_cases'] = data_train_iq['total_cases'].astype('float')

corr = data_train_iq.corr()
fig = plt.figure(figsize=(40,40))
ax = fig.add_subplot(111)
cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = np.arange(0,len(data_train_iq.columns),1)
ax.set_xticks(ticks)
plt.xticks(rotation=90)
ax.set_yticks(ticks)
ax.set_xticklabels(data_train_iq.columns)
ax.set_yticklabels(data_train_iq.columns)
plt.show()

"""# Plot Of The Correlation Matrix For SJ"""

# plot correlation matrix for Iq  (Mean Imputed Values)
data_train_sj['total_cases'] = data_train_sj['total_cases'].astype('float')
corr = data_train_sj.corr()
fig = plt.figure(figsize=(40,40))
ax = fig.add_subplot(111)
cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = np.arange(0,len(data_train_sj.columns),1)
ax.set_xticks(ticks)
plt.xticks(rotation=90)
ax.set_yticks(ticks)
ax.set_xticklabels(data_train_sj.columns)
ax.set_yticklabels(data_train_sj.columns)
plt.show()

"""# Bar plot for IQ"""

(corr_iq.total_cases.drop('total_cases').sort_values(ascending=False).plot.barh())

"""# Bar Plot For SJ"""

(corr_sj.total_cases.drop('total_cases').sort_values(ascending=False).plot.barh())

"""# Convert dataframes to numpy arrays"""

# Convert all the data to numpy arrays to use in our functions
X_data_train_sj = X_data_train_sj.to_numpy()
X_data_train_iq = X_data_train_iq.to_numpy()
y_data_train_sj = y_data_train_sj.to_numpy()
y_data_train_iq = y_data_train_iq.to_numpy()
X_data_test_sj = X_data_test_sj.to_numpy()
X_data_test_iq = X_data_test_iq.to_numpy()
X_train_regression_imputed_sj = X_train_regression_imputed_sj.to_numpy()
X_train_regression_imputed_iq = X_train_regression_imputed_iq.to_numpy()

# Shape check
assert X_data_train_sj.shape[1] == X_data_train_iq.shape[1]
assert X_data_train_sj.shape[0] == y_data_train_sj.shape[0]
assert X_data_test_sj.shape[1] == X_data_test_iq.shape[1]
assert X_data_train_iq.shape[0] == y_data_train_iq.shape[0]

"""# OLS, Ridge, Elastic Net, Kernel Regression without cross validation"""

def LinearRegressionOLS(X,y, X_test):
    LinearRegression = linear_model.LinearRegression
    lin_reg = LinearRegression()
    lin_reg.fit(X,y)
    y_pred = lin_reg.predict(X_test)
    return lin_reg, y_pred

def RidgeRegression(X, y, alp, X_test):
    Ridge  = linear_model.Ridge
    ridge_reg =  Ridge(alpha = alp, solver = "cholesky")
    ridge_reg.fit(X,y)
    y_pred = ridge_reg.predict(X_test)
    return ridge_reg, y_pred

def LassoRegression(X, y, alp, X_test):
    Lasso = linear_model.Lasso
    lasso_reg  = Lasso(alpha = alp)
    lasso_reg.fit(X, y)
    y_pred = lasso_reg.predict(X_test)
    return lasso_reg, y_pred

def ElasticNetRegression(X, y, alp, ratio_l1, X_test):
    ElasticNet = linear_model.ElasticNet
    elastic_net = ElasticNet(alpha = alp, l1_ratio = ratio_l1) 
    elastic_net.fit(X, y)
    y_pred = elastic_net.predict(X_test)
    return elastic_net, y_pred


def KernelRegression(X, y, alp, X_test):
    KernelReg = sklearn.kernel_ridge.KernelRidge
    kernel_reg = KernelReg(alpha = alp)
    kernel_reg.fit(X,y)
    y_pred = kernel_reg.predict(X_test)
    return kernel_reg, y_pred


def RandomForestRegression(X, y, num_estimators, max_depth, X_test):
    RandomForestRegressor = ensemble.RandomForestRegressor
    random_forest_regressor = RandomForestRegressor(n_estimators= num_estimators, criterion = 'mae', max_depth=max_depth)
    random_forest_regressor.fit(X, y)
    y_pred = random_forest_regressor.predict(X_test)
    return random_forest_regressor, y_pred

def GradientBoostingRegression(X,y , num_estimators, learning_r, maximum_depth, X_test):
    GradientBoostingRegressor = ensemble.GradientBoostingRegressor
    gradient_boosting_regressor = GradientBoostingRegressor(loss='ls', n_estimators= num_estimators, max_depth= maximum_depth, learning_rate = learning_r)
    gradient_boosting_regressor.fit(X, y)
    y_pred = gradient_boosting_regressor.predict(X_test)
    return gradient_boosting_regressor, y_pred

def BaggingRegressor(X, y, X_test, n_estimators=16):
    BaggingRegressor = ensemble.BaggingRegressor
    bagging_regressor = BaggingRegressor(n_estimators=n_estimators)
    bagging_regressor.fit(X,y)
    y_pred = bagging_regressor.predict(X_test)
    return bagging_regressor, y_pred

"""# OLS, Ridge, Elastic Net, Kernel Regression **with** cross validation of hyperparameters"""

# alps MUST be passed in as a array e.g. apls [0.1]
def RidgeRegressionCV(X, y,kfolds_num, alps, X_test):
    Ridge  = linear_model.RidgeCV
    ridge_reg =  Ridge(alphas = alps, cv = kfolds_num, scoring="neg_mean_absolute_error")
    ridge_reg.fit(X,y)
    y_pred = ridge_reg.predict(X_test)
    return ridge_reg, y_pred

def LassoRegressionCV(X, y, kfolds_num, alps, X_test):
    Lasso = linear_model.LassoCV
    lasso_reg  = Lasso(alphas = alps, cv = kfolds_num)
    lasso_reg.fit(X, y)
    y_pred = lasso_reg.predict(X_test)
    return lasso_reg, y_pred

def ElasticNetRegression(X, y, alps, l1_ratios_to_try, kfolds_num, X_test):
    # l1_ratios_to_try = [.1, .5, .7, .9, .95, .99, 1]
    ElasticNet = linear_model.ElasticNetCV
    elastic_net = ElasticNet(l1_ratio = l1_ratios_to_try, alphas = alps, cv = kfolds_num) 
    elastic_net.fit(X, y)
    y_pred = elastic_net.predict(X_test)
    return elastic_net, y_pred


def KernelRegressionCV(X, y, alps, n_comp_pca, X_test):
    KernelReg = sklearn.kernel_ridge.KernelRidge
    kernel_reg = KernelReg(alpha = alps)
    pca = sklearn.decomposition.PCA(n_components=n_comp_pca)
    X = pca.fit_transform(X)
    for x in X:
      print(x)
    kernel_reg.fit(X,y)
    y_pred = kernel_reg.predict(X_test)
    return kernel_reg, y_pred

# X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
# # y = 1 * x_0 + 2 * x_1 + 3
# y = np.dot(X, np.array([1, 2])) + 3
# print(y)
# print(KernelRegressionCV(X, y, [0.1, 0.2], X))
# plt.plot(y, color='red')
# plt.scatter(X[:,0], X[:,1])
# plt.title('Model from Full Training Data')
# plt.show()

"""# Main Code"""

with_CV = True

if not with_CV:

  prediction_function = BaggingRegressor
  prediction_function_str = "BaggingRegressor"
  # num_estimators, learning_r, maximum_depth,
  # num_estimators=100, learning_r = 0.1, maximum_depth=3
  models_sj = []
  accuracies_mean_sj = []
  #Method 01 (Using Mean Imputed Values) 
  kf = KFold(n_splits=5)
  for train_index, test_index in kf.split(X_data_train_sj):
    X_tr, X_te = X_data_train_sj[train_index], X_data_train_sj[test_index]
    y_tr, y_te = y_data_train_sj[train_index], y_data_train_sj[test_index]
    model_sj, y_pred = prediction_function(X_tr, y_tr, X_test=X_te)
    accuracies_mean_sj.append(sklearn.metrics.mean_absolute_error(y_pred, y_te))
    models_sj.append(model_sj)
    #print(y_pred)
  print('mean imputed, sj: ',sum(accuracies_mean_sj) / 5)

  filename = "models_" + prediction_function_str + "_mean_sj.pickle"
  path = F"/content/gdrive/My Drive/models/{filename}" 
  pickle.dump(models_sj, open(path, 'wb'))

  models_iq = []
  accuracies_mean_iq = []
  kf = KFold(n_splits=5)
  for train_index, test_index in kf.split(X_data_train_iq):
    X_tr, X_te = X_data_train_iq[train_index], X_data_train_iq[test_index]
    y_tr, y_te = y_data_train_iq[train_index], y_data_train_iq[test_index]
    model_iq, y_pred = prediction_function(X_tr, y_tr, X_test=X_te)
    accuracies_mean_iq.append(sklearn.metrics.mean_absolute_error(y_pred, y_te))
    models_iq.append(model_iq)
  print('mean imputed, iq: ',sum(accuracies_mean_iq) / 5)

  filename = "models_" + prediction_function_str + "_mean_iq.pickle"
  path = F"/content/gdrive/My Drive/models/{filename}" 
  pickle.dump(models_iq, open(path, 'wb'))

# Method 02 (using Regression Imputed Values)

  models_sj = []
  accuracies_regr_sj = []
  kf = KFold(n_splits=5)
  for train_index, test_index in kf.split(X_train_regression_imputed_sj):
    X_tr, X_te = X_train_regression_imputed_sj[train_index], X_train_regression_imputed_sj[test_index]
    y_tr, y_te = y_data_train_sj[train_index], y_data_train_sj[test_index]
    model_sj, y_pred = prediction_function(X_tr, y_tr, X_test=X_te)
    accuracies_regr_sj.append(sklearn.metrics.mean_absolute_error(y_pred, y_te))
    models_sj.append(model_sj)
  print('regression imputed, sj: ',sum(accuracies_regr_sj) / 5)

  filename = "models_" + prediction_function_str + "_regression_sj.pickle"
  path = F"/content/gdrive/My Drive/models/{filename}" 
  pickle.dump(models_sj, open(path, 'wb'))

  models_iq = []
  accuracies_regr_iq = []
  kf = KFold(n_splits=5)
  for train_index, test_index in kf.split(X_train_regression_imputed_iq):
    X_tr, X_te = X_train_regression_imputed_iq[train_index], X_train_regression_imputed_iq[test_index]
    y_tr, y_te = y_data_train_iq[train_index], y_data_train_iq[test_index]
    model_iq, y_pred = prediction_function(X_tr, y_tr, X_test=X_te)
    accuracies_regr_iq.append(sklearn.metrics.mean_absolute_error(y_pred, y_te))
    models_iq.append(model_iq)
  print('regression imputed, iq:',sum(accuracies_regr_iq) / 5)

  filename = "models_" + prediction_function_str + "_regression_iq.pickle"
  path = F"/content/gdrive/My Drive/models/{filename}" 
  pickle.dump(models_sj, open(path, 'wb'))

else:
  prediction_function_CV = RidgeRegressionCV
  prediction_function_CV_str = "RidgeRegressionCV"
  n_pca = 2

  models_sj = []
  accuracies_mean_sj = []
  kf = KFold(n_splits=5)
  for train_index, test_index in kf.split(X_data_train_sj):
    X_tr, X_te = X_data_train_sj[train_index], X_data_train_sj[test_index]
    y_tr, y_te = y_data_train_sj[train_index], y_data_train_sj[test_index]
    model_sj, y_pred = prediction_function_CV(X_tr, y_tr, alps=[x/10 for x in range(0,100,5)], X_test=X_te, kfolds_num=5)
    accuracies_mean_sj.append(sklearn.metrics.mean_absolute_error(y_pred, y_te))
    models_sj.append(model_sj)
  print('mean imputed, sj: ',sum(accuracies_mean_sj) / 5)

  filename = "models_" + prediction_function_CV_str + "_mean_sj.pickle"
  path = F"/content/gdrive/My Drive/models/{filename}" 
  pickle.dump(models_sj, open(path, 'wb'))

  models_iq = []
  accuracies_mean_iq = []
  kf = KFold(n_splits=5)
  for train_index, test_index in kf.split(X_data_train_iq):
    X_tr, X_te = X_data_train_iq[train_index], X_data_train_iq[test_index]
    y_tr, y_te = y_data_train_iq[train_index], y_data_train_iq[test_index]
    model_iq, y_pred = prediction_function_CV(X_tr, y_tr,alps=[x/10 for x in range(0,100,5)], X_test=X_te, kfolds_num=5)
    accuracies_mean_iq.append(sklearn.metrics.mean_absolute_error(y_pred, y_te))
  print('mean imputed, iq: ', sum(accuracies_mean_iq) / 5)

  filename = "models_" + prediction_function_CV_str + "_mean_iq.pickle"
  path = F"/content/gdrive/My Drive/models/{filename}" 
  pickle.dump(models_iq, open(path, 'wb'))

  # regression imputed

  models_sj = []
  accuracies_regr_sj = []
  kf = KFold(n_splits=5)
  for train_index, test_index in kf.split(X_train_regression_imputed_sj):
    X_tr, X_te = X_train_regression_imputed_sj[train_index], X_train_regression_imputed_sj[test_index]
    y_tr, y_te = y_data_train_sj[train_index], y_data_train_sj[test_index]
    model_sj, y_pred = prediction_function_CV(X_tr, y_tr, alps=[x/10 for x in range(0,100,5)], X_test=X_te, kfolds_num=5)
    accuracies_regr_sj.append(sklearn.metrics.mean_absolute_error(y_pred, y_te))
    models_sj.append(model_sj)
  print('regression imputed, sj: ',sum(accuracies_regr_sj) / 5)

  filename = "models_" + prediction_function_CV_str + "_regression_sj.pickle"
  path = F"/content/gdrive/My Drive/models/{filename}" 
  pickle.dump(models_sj, open(path, 'wb'))

  models_iq = []
  accuracies_regr_iq = []
  kf = KFold(n_splits=5)
  for train_index, test_index in kf.split(X_train_regression_imputed_iq):
    X_tr, X_te = X_train_regression_imputed_iq[train_index], X_train_regression_imputed_iq[test_index]
    y_tr, y_te = y_data_train_iq[train_index], y_data_train_iq[test_index]
    model_iq, y_pred = prediction_function_CV(X_tr, y_tr, alps=[x/10 for x in range(0,100,5)], X_test=X_te, kfolds_num=5)
    accuracies_regr_iq.append(sklearn.metrics.mean_absolute_error(y_pred, y_te))
    models_iq.append(model_iq)
  print('regression imputed, iq: ',sum(accuracies_regr_iq) / 5)

  filename = "models_" + prediction_function_CV_str + "_regression_iq.pickle"
  path = F"/content/gdrive/My Drive/models/{filename}" 
  pickle.dump(models_sj, open(path, 'wb'))

"""# Scores:
OLS:

*   mean imputed, sj:  29.168905913415927
*   mean imputed, iq:  6.8399866472402095
*   regression imputed, sj:  28.890970566353047
*   regression imputed, iq: 6.8193495595292175

Ridge:
*   mean imputed, sj:  28.920965432063394
*   mean imputed, iq:  6.808963514214744
*   regression imputed, sj:  28.835143676312832
*   regression imputed, iq:  6.785164877842851

Lasso:
*   mean imputed, sj:  28.77430125517251
*   mean imputed, iq:  6.8133435316067885
*   regression imputed, sj:  28.761707693701375
*   regression imputed, iq:  6.8266466087443

ElasticNet:
*   mean imputed, sj:  28.77632110482046
*   mean imputed, iq:  6.815438109004431
*   regression imputed, sj:  28.78797287579158
*   regression imputed, iq:  6.817201055489536

RandomForests, 100 estimators:
*   mean imputed, sj:  32.90370266810786
*   mean imputed, iq:  7.596048076923077
*   regression imputed, sj:  33.296109398111284
*   regression imputed, iq: 7.68326923076923

RandomForests, 1000 estimators:
*   mean imputed, sj:  32.66080130560929
*   mean imputed, iq:  7.683057692307694
*   regression imputed, sj:  33.00587241722608
*   regression imputed, iq: 7.67959423076923

GradientBoosting:
*   mean imputed, sj:  31.098715266463046
*   mean imputed, iq:  7.637982854220853
*   regression imputed, sj:  31.88351265001993
*   regression imputed, iq: 7.608261596794124

RandomForest optimal:
*   mean imputed, sj:  33.164747773559924
*   mean imputed, iq:  7.779050116550115
*   regression imputed, sj:  33.14519138937446
*   regression imputed, iq: 7.715238927738928

BaggingTrees, optimal:
*   mean imputed, sj:  33.04494143958357
*   mean imputed, iq:  7.54142342032967
*   regression imputed, sj:  33.4509787328858
*   regression imputed, iq: 7.584032451923077
"""

# get hyperparams
prediction_function_str = "GradientBoostingRegression"
filename = "models_" + prediction_function_str + "_regression_sj.pickle"
path = F"/content/gdrive/My Drive/models/{filename}" 
models_sj = pickle.load(open(path, "rb"))
model = models_sj[0]
print(model.get_params)

y_pred = np.array(len(X_test_regression_imputed_sj) + len(X_test_regression_imputed_iq))
# predict values
#models = pickle.load()
prediction_function_str = "RidgeRegressionCV"
filename = "models_" + prediction_function_str + "_regression_sj.pickle"
path = F"/content/gdrive/My Drive/models/{filename}" 
models_sj = pickle.load(open(path, "rb"))
filename = "models_" + prediction_function_str + "_regression_iq.pickle"
path = F"/content/gdrive/My Drive/models/{filename}" 
models_iq = pickle.load(open(path, "rb"))

y_pred_sj = np.zeros(len(X_test_regression_imputed_sj))
for model in models_sj:
  y_pred_sj += model.predict(X_test_regression_imputed_sj)
print(y_pred_sj)
y_pred_sj = y_pred_sj/5
print(y_pred_sj)

y_pred_iq = np.zeros(len(X_test_regression_imputed_iq))
for model in models_sj:
  y_pred_iq += model.predict(X_test_regression_imputed_iq)
y_pred_iq = y_pred_iq/5

save_filename = 'y_pred.csv'
save_path = F"/content/gdrive/My Drive/models/{save_filename}"
y_pred = np.concatenate([y_pred_sj, y_pred_iq])
y_pred = y_pred.reshape((len(y_pred),1))
np.savetxt(save_path, y_pred, delimiter=',', fmt='%d')

sorted(sklearn.metrics.SCORERS.keys())

maes_sj = []
maes_iq = []
ns = [10, 50, 100, 200, 500, 1000]
for n_estimators in ns:
  regr_sj = sklearn.ensemble.RandomForestRegressor(n_estimators=n_estimators, max_depth=5)
  regr_iq = sklearn.ensemble.RandomForestRegressor(n_estimators=n_estimators, max_depth=2)
  regr_sj.fit(X_train_regression_imputed_sj, y_data_train_sj)
  regr_iq.fit(X_train_regression_imputed_iq, y_data_train_iq)
  y_pred_sj = regr_sj.predict(X_train_regression_imputed_sj)
  y_pred_iq = regr_iq.predict(X_train_regression_imputed_iq)
  maes_sj.append(sklearn.metrics.mean_absolute_error(y_data_train_sj, y_pred_sj))
  maes_iq.append(sklearn.metrics.mean_absolute_error(y_data_train_iq, y_pred_iq))

plt.plot(ns, maes_sj, label="San Juan")
plt.plot(ns, maes_iq, label="Iquitos")
plt.title("Training MAE for different n_estimators")
plt.xlabel("n_estimators")
plt.ylabel("MAE")
plt.legend()